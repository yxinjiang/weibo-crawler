{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta,date\n",
    "import locale\n",
    "import re \n",
    "import os \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import random \n",
    "import sys\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_followers_id(soup,url_str='https://weibo.cn'):\n",
    "    user_names = [a.get_text() for a in soup.find_all('a', href=True) if url_str in a['href'] and a.get_text()]\n",
    "    user_uids = [a['href'].split('/')[-1] for a in soup.find_all('a', href=True) if url_str in a['href'] and a.get_text()]\n",
    "    user_uids = [item for item in user_uids if not '=' in item or '.' in item]\n",
    "    return user_names, user_uids\n",
    "\n",
    "def save_array2txt(my_array,filename):\n",
    "    # Open the file for writing\n",
    "    with open(filename, 'w') as file:\n",
    "        for item in my_array:\n",
    "        # Write the string representation of the array to the file\n",
    "            file.write(item)\n",
    "            file.write('\\n')\n",
    "    return True\n",
    "\n",
    "def random_wait(min_seconds=15,max_seconds=30):\n",
    "    random_seconds = random.randint(min_seconds, max_seconds)\n",
    "    time.sleep(random_seconds)\n",
    "    print(f'get next page after {random_seconds} seconds')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('cookies.txt', 'r', encoding='utf-8') as f:\n",
    "    cookie = f.read().strip()\n",
    "\n",
    "# 设置请求头部信息\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Cookie': cookie\n",
    "}\n",
    "\n",
    "# 定义需要抓取关注列表的微博用户ID和页数\n",
    "user_id = '6235235201'\n",
    "page = 1\n",
    "user_names = []\n",
    "user_uids = []\n",
    "\n",
    "\n",
    "profile_url = f'https://weibo.cn/{user_id}/profile'\n",
    "profile_response = requests.get(profile_url, headers=headers)\n",
    "profile_soup = BeautifulSoup(profile_response.content, 'html.parser')\n",
    "user_name = profile_soup.find('title').text.split(' ')[0][:-3]\n",
    "print(user_name)\n",
    "\n",
    "\n",
    "\n",
    "# 构造请求URL\n",
    "#url = f'https://weibo.cn/u/{user_id}?page={page}'\n",
    "url = f'https://weibo.cn/{user_id}/follow?page={page}'\n",
    "# 发送请求\n",
    "response = requests.get(url, headers=headers)\n",
    "random_wait()\n",
    "\n",
    "# 解析HTML页面并提取关注列表\n",
    "soup = BeautifulSoup(response.content, 'html.parser') #html5lib  \n",
    "followings = get_followers_id(soup)\n",
    "user_names += followings[0]\n",
    "user_uids += followings[1]\n",
    "\n",
    "random_wait()\n",
    "\n",
    "page_next = soup.find('a', string='下页')\n",
    "while page_next:\n",
    "    # 获取下一页的URL\n",
    "    page += 1\n",
    "    url = f'https://weibo.cn/{user_id}/follow?page={page}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    followings = get_followers_id(soup)\n",
    "    user_names += followings[0]\n",
    "    user_uids += followings[1]\n",
    "    page_next = soup.find('a', string='下页')\n",
    "    random_wait()\n",
    "\n",
    "user_names = list(set(user_names))\n",
    "user_uids = list(set(user_uids))\n",
    "\n",
    "save_array2txt(user_names,f'./following_lists/{user_id}_{user_name}_followers_names.txt')\n",
    "save_array2txt(user_uids,f'./following_lists/{user_id}_{user_name}_followers_uids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date(created_at):\n",
    "        \"\"\"标准化微博发布时间\"\"\"\n",
    "        if \"刚刚\" in created_at:\n",
    "            ts = datetime.now()\n",
    "        elif \"分钟\" in created_at:\n",
    "            #print('find 分钟')\n",
    "            minute = created_at[: created_at.find(\"分钟\")]\n",
    "            minute = timedelta(minutes=int(minute))\n",
    "            ts = datetime.now() - minute\n",
    "        elif \"小时\" in created_at:\n",
    "            #print('find 小时')\n",
    "            hour = created_at[: created_at.find(\"小时\")]\n",
    "            hour = timedelta(hours=int(hour))\n",
    "            ts = datetime.now() - hour\n",
    "        elif \"今天\" in created_at:  \n",
    "            minute = created_at[3:8]\n",
    "            #print(f'find 今天 {minute}') \n",
    "            date_str = datetime.now().strftime('%Y-%m-%d') \n",
    "            ts = datetime.strptime(f'{date_str} {minute}','%Y-%m-%d %H:%M')\n",
    "        elif \"昨天\" in created_at:\n",
    "            #print('find 昨天')\n",
    "            day = timedelta(days=1)\n",
    "            ts = datetime.now() - day\n",
    "        else:\n",
    "            #print('none')\n",
    "            date_str,time_str = created_at.split(' ')[0],created_at.split(' ')[1]\n",
    "            time_str = time_str[:5]\n",
    "            year_str = str(datetime.now().year)\n",
    "            ts = datetime.strptime(f'{year_str}年{date_str} {time_str}','%Y年%m月%d日 %H:%M')\n",
    "\n",
    "        return ts\n",
    "\n",
    "def standardize_info(weibo):\n",
    "    \"\"\"标准化信息，去除乱码\"\"\"\n",
    "    for k, v in weibo.items():\n",
    "        if (\n",
    "            \"bool\" not in str(type(v))\n",
    "            and \"int\" not in str(type(v))\n",
    "            and \"list\" not in str(type(v))\n",
    "            and \"long\" not in str(type(v))\n",
    "        ):\n",
    "            weibo[k] = (\n",
    "                v.replace(\"\\u200b\", \"\")\n",
    "                .encode(sys.stdout.encoding, \"ignore\")\n",
    "                .decode(sys.stdout.encoding)\n",
    "            )\n",
    "    return weibo\n",
    "\n",
    "def download_image(url, filename):\n",
    "    \"\"\"\n",
    "    下载图片\n",
    "    \"\"\"\n",
    "    filename = Path(filename)\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "def download_video(url, path):\n",
    "    \"\"\"\n",
    "    下载视频\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(os.path.join(path, os.path.basename(url)), 'wb') as f:\n",
    "            for chunk in r.iter_content(1024*1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "def get_comments(comment_url,headers):\n",
    "    comment_response = requests.get(comment_url, headers=headers)\n",
    "    comment_soup = BeautifulSoup(comment_response.content, 'html.parser')\n",
    "    comment_text = [a.get_text() for a in comment_soup.find_all('span', class_='ctt') if a.get_text()]\n",
    "    comment_count_get = len(comment_text)#int(comment_text.split('[')[-1].split(']')[0])\n",
    "    return comment_count_get,comment_text[1:]\n",
    "     \n",
    "\n",
    "def get_page_content(user_id,page,headers,curr_time,time_limit,user_folder,original_pic_download=False):\n",
    "    next_page = True\n",
    "    # 构造请求URL\n",
    "    url = f'https://weibo.cn/{user_id}?filter=1&page={page}'\n",
    "\n",
    "    # 发送请求\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # 解析HTML页面并提取原创微博\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    weibos = soup.find_all('div', class_='c', id=True)\n",
    "\n",
    "    weibo_content = []\n",
    "\n",
    "    # 输出原创微博\n",
    "    for weibo in weibos:\n",
    "        # 提取微博发布时间\n",
    "        create_time_str = weibo.find('span', class_='ct').get_text()\n",
    "        print(f'create_at: {create_time_str}')\n",
    "        create_at = standardize_date(create_time_str)\n",
    "        if create_at > time_limit:\n",
    "            # 提取微博正文\n",
    "            content = weibo.find('span', class_='ctt').get_text()\n",
    "            comment_count_text = weibo.find('a',href=True,class_='cc').get_text()\n",
    "            comment_count = re.search(r'\\[(\\d+)\\]', comment_count_text).group(1)\n",
    "            comment_url = weibo.find('a',href=True,class_='cc')['href']\n",
    "            comment_count_get,comment_text = get_comments(comment_url,headers)\n",
    "\n",
    "            weibo_content.append({'create_at':create_at,'content':content,'comment_count':comment_count,'comment_count_get':comment_count_get,'comment_text':comment_text})\n",
    "            print(content)\n",
    "            if original_pic_download:\n",
    "                imgs = weibo.find_all('img', alt='图片')\n",
    "                for i,img in enumerate(imgs):\n",
    "                    img_url = img.get('src')\n",
    "                    image_folder = Path(os.path.join(user_folder,'img'))\n",
    "                    image_folder.mkdir(parents=True,exist_ok=True)\n",
    "                    file_name = os.path.join(image_folder,f'{user_id}_{time.strftime(\"%Y%m%d%H%M%S\")}_{i}.jpg')\n",
    "                    download_image(img_url, file_name)\n",
    "\n",
    "        else:\n",
    "            print('get last weibo before: ',create_at)\n",
    "            next_page = False\n",
    "            break\n",
    "        page_next = soup.find('a', string='下页')\n",
    "        if not len(page_next):\n",
    "            next_page = False\n",
    "    return next_page,weibo_content\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "茶美丽的追剧日常\n",
      "get next page after 10 seconds\n",
      "get weibos until 03月18日 09:14\n",
      "create_at: 3分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "CPB才是今天真流量的试金石吧？首先这个牌子8紫都没代言，比较客观。其次这个牌子是蔡顶流代言，捍卫本属于自己的local非常合理。就结果来说：第一依旧是肖顶流，这个没什么疑问，蔡顶流只要拿到第二，我认为就算可以了，你们能接受我这个标准吗？ \n",
      "create_at: 12分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "男艺人签到人数前五：肖战、王一博、易烊千玺、王俊凯、龚俊女艺人签到人数前三：刘雨昕、迪丽热巴、鞠婧祎这个签到是不是能看出真实的粉丝体量？ \n",
      "create_at: 14分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "#迪丽热巴流光闪烁花絮# 来看看迪丽热巴超近距离直拍，那个甩头发回眸一笑，路人都尖叫了 \n",
      "create_at: 43分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "#杨洋白西服#杨洋的妆造，always给我一种敷衍的赶脚，always白色系，他真的好爱白色。咋说呢？白色系有的时候太像男g关了，会把他本身的贵气压制，时尚圈一般是很少用一身白色系的，杨洋对白色系的执迷，我表示真滴很：敬佩。 \n",
      "create_at: 45分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "#人生路遥改名人生之路#李沁的大青衣之路，即将开启！她后面还有和肖战合作的主流剧《梦中的那片海》、和王一博合作的主流剧《光明之路》，可以说流量+主流双保险！后续弯道超车，不是没可能的！ \n",
      "create_at: 48分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "#迪丽热巴流光闪烁花絮#这就是所谓的缘分吧[滑稽] \n",
      "create_at: 51分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "#光渊#听说这个剧被点名批评了啊但播还是可以播的。 \n",
      "create_at: 今天 09:32 来自沦落民间的iPhone 13(粉色)\n",
      "蔡徐坤这个大C位是不是拿的也不是多光彩。其他人没什么咖的赶脚。 \n",
      "create_at: 今天 09:31 来自沦落民间的iPhone 13(粉色)\n",
      "纯甄热转数据统计前十杨紫是唯一的女艺人是不是因为她代言这个，所以爱丽丝不想给她代言的产品贡献热度？ \n",
      "create_at: 今天 09:19 来自沦落民间的iPhone 13(粉色)\n",
      "扁桃体发炎了🙃有没有推荐的药🙃 \n",
      "get next page after 10 seconds\n",
      "get content from page 2\n",
      "create_at: 03月17日 23:41\n",
      "get last weibo before:  2023-03-17 23:41:00\n"
     ]
    }
   ],
   "source": [
    "# 设置请求头部信息和Cookie\n",
    "with open('cookies.txt', 'r', encoding='utf-8') as f:\n",
    "    cookie = f.read().strip()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Cookie': cookie\n",
    "}\n",
    "\n",
    "# 定义需要抓取的微博用户ID和页数\n",
    "user_id = '6028429521'\n",
    "original_pic_download=True\n",
    "page = 1\n",
    "hours = 2\n",
    "next_page = True\n",
    "\n",
    "profile_url = f'https://weibo.cn/{user_id}/profile'\n",
    "profile_response = requests.get(profile_url, headers=headers)\n",
    "profile_soup = BeautifulSoup(profile_response.content, 'html.parser')\n",
    "user_name = profile_soup.find('title').text.split(' ')[0][:-3]\n",
    "print(user_name)\n",
    "random_wait()\n",
    "\n",
    "user_folder = Path(os.path.join(os.getcwd(),'weibo',user_name))\n",
    "user_folder.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# 计算最近n小时的时间\n",
    "curr_time = datetime.now()\n",
    "time_limit = curr_time - timedelta(hours=hours)\n",
    "print('get weibos until',time_limit.strftime('%m月%d日 %H:%M'))\n",
    "today_str = time_limit.strftime('%m月%d日')\n",
    "all_weibo_content = []\n",
    "next_page,weibo_content = get_page_content(user_id,page,headers,curr_time,time_limit,user_folder)\n",
    "all_weibo_content.extend(weibo_content)\n",
    "random_wait()\n",
    "\n",
    "while(next_page):\n",
    "    # 获取下一页的URL\n",
    "    page += 1\n",
    "    print(f'get content from page {page}')\n",
    "    next_page,weibo_content = get_page_content(user_id,page,headers,curr_time,time_limit,user_folder)\n",
    "    if next_page==False:\n",
    "        break\n",
    "    all_weibo_content.extend(weibo_content)\n",
    "    print(f'find next page {next_page}')\n",
    "    random_wait()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>create_at</th>\n",
       "      <th>content</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_count_get</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>user_uid</th>\n",
       "      <th>user_name</th>\n",
       "      <th>last_check</th>\n",
       "      <th>check_before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-18 11:11:56.183435</td>\n",
       "      <td>CPB才是今天真流量的试金石吧？首先这个牌子8紫都没代言，比较客观。其次这个牌子是蔡顶流代言...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[铁粉1就很难压了其实，还没试过金粉牌什么感觉[欢乐熊出没], 别急，等着看热闹！看心情交吃...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-18 11:02:56.681155</td>\n",
       "      <td>男艺人签到人数前五：肖战、王一博、易烊千玺、王俊凯、龚俊女艺人签到人数前三：刘雨昕、迪丽热巴...</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>[gj就算了吧，他最氵，生日月签到比其他时间多了20万, 不一定，听说这个也能买，我记得以前...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-18 11:00:57.315615</td>\n",
       "      <td>#迪丽热巴流光闪烁花絮# 来看看迪丽热巴超近距离直拍，那个甩头发回眸一笑，路人都尖叫了</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[确实漂亮，就是把美女冻着了，我看着都发抖, 太美了，很難忍住不尖叫[送花花], 生图都美翻...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-18 10:31:57.794744</td>\n",
       "      <td>#杨洋白西服#杨洋的妆造，always给我一种敷衍的赶脚，always白色系，他真的好爱白色...</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>[品牌活动，跟杨洋有啥关系，没话题就别硬蹭，杨洋想穿什么颜色穿什么颜色，他穿啥都好看。 评论...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-18 10:29:58.576493</td>\n",
       "      <td>#人生路遥改名人生之路#李沁的大青衣之路，即将开启！她后面还有和肖战合作的主流剧《梦中的那片...</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>[你男朋友也出演了, 她又不是一番，肖燕一番收视率1.5，不是照样还是三线演员, 你想多了，...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-03-18 10:26:59.321782</td>\n",
       "      <td>#迪丽热巴流光闪烁花絮#这就是所谓的缘分吧[滑稽]</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>[回复@g-junya201909:紫设计师粉丝画的礼服画稿这个气质…, 迪丽热巴独美 评论...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-03-18 10:23:59.869476</td>\n",
       "      <td>#光渊#听说这个剧被点名批评了啊但播还是可以播的。</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[拉倒吧，真不让播还能预热几天，自己炒作还要甩锅, 这剧还有人看？, 拉到, 回复@山月不知...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-03-18 09:32:00.000000</td>\n",
       "      <td>蔡徐坤这个大C位是不是拿的也不是多光彩。其他人没什么咖的赶脚。</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>[你就说这几个人里面，哪个不比他能力强？, 他旁边的那几位在歌手里咖位不低啊。容祖儿算是新香...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-03-18 09:31:00.000000</td>\n",
       "      <td>纯甄热转数据统计前十杨紫是唯一的女艺人是不是因为她代言这个，所以爱丽丝不想给她代言的产品贡献...</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>[一个简单的逻辑问题，热转100万+，如果真有这么多人，不可能一个热评都进不去，之所以一个都...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-03-18 09:19:00.000000</td>\n",
       "      <td>扁桃体发炎了🙃有没有推荐的药🙃</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>[蒲地蓝, 你昨天太挑事了，回旋镖了呗, 扁桃体就用抗生素效果最好，不化脓不要用喷剂，基本上...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-18 09:14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   create_at  \\\n",
       "0 2023-03-18 11:11:56.183435   \n",
       "1 2023-03-18 11:02:56.681155   \n",
       "2 2023-03-18 11:00:57.315615   \n",
       "3 2023-03-18 10:31:57.794744   \n",
       "4 2023-03-18 10:29:58.576493   \n",
       "5 2023-03-18 10:26:59.321782   \n",
       "6 2023-03-18 10:23:59.869476   \n",
       "7 2023-03-18 09:32:00.000000   \n",
       "8 2023-03-18 09:31:00.000000   \n",
       "9 2023-03-18 09:19:00.000000   \n",
       "\n",
       "                                             content comment_count  \\\n",
       "0  CPB才是今天真流量的试金石吧？首先这个牌子8紫都没代言，比较客观。其次这个牌子是蔡顶流代言...             4   \n",
       "1  男艺人签到人数前五：肖战、王一博、易烊千玺、王俊凯、龚俊女艺人签到人数前三：刘雨昕、迪丽热巴...            22   \n",
       "2       #迪丽热巴流光闪烁花絮# 来看看迪丽热巴超近距离直拍，那个甩头发回眸一笑，路人都尖叫了              9   \n",
       "3  #杨洋白西服#杨洋的妆造，always给我一种敷衍的赶脚，always白色系，他真的好爱白色...            35   \n",
       "4  #人生路遥改名人生之路#李沁的大青衣之路，即将开启！她后面还有和肖战合作的主流剧《梦中的那片...            19   \n",
       "5                         #迪丽热巴流光闪烁花絮#这就是所谓的缘分吧[滑稽]              6   \n",
       "6                         #光渊#听说这个剧被点名批评了啊但播还是可以播的。             14   \n",
       "7                   蔡徐坤这个大C位是不是拿的也不是多光彩。其他人没什么咖的赶脚。             76   \n",
       "8  纯甄热转数据统计前十杨紫是唯一的女艺人是不是因为她代言这个，所以爱丽丝不想给她代言的产品贡献...            62   \n",
       "9                                   扁桃体发炎了🙃有没有推荐的药🙃             62   \n",
       "\n",
       "   comment_count_get                                       comment_text  \\\n",
       "0                  5  [铁粉1就很难压了其实，还没试过金粉牌什么感觉[欢乐熊出没], 别急，等着看热闹！看心情交吃...   \n",
       "1                 11  [gj就算了吧，他最氵，生日月签到比其他时间多了20万, 不一定，听说这个也能买，我记得以前...   \n",
       "2                 10  [确实漂亮，就是把美女冻着了，我看着都发抖, 太美了，很難忍住不尖叫[送花花], 生图都美翻...   \n",
       "3                 12  [品牌活动，跟杨洋有啥关系，没话题就别硬蹭，杨洋想穿什么颜色穿什么颜色，他穿啥都好看。 评论...   \n",
       "4                 13  [你男朋友也出演了, 她又不是一番，肖燕一番收视率1.5，不是照样还是三线演员, 你想多了，...   \n",
       "5                  7  [回复@g-junya201909:紫设计师粉丝画的礼服画稿这个气质…, 迪丽热巴独美 评论...   \n",
       "6                 14  [拉倒吧，真不让播还能预热几天，自己炒作还要甩锅, 这剧还有人看？, 拉到, 回复@山月不知...   \n",
       "7                 13  [你就说这几个人里面，哪个不比他能力强？, 他旁边的那几位在歌手里咖位不低啊。容祖儿算是新香...   \n",
       "8                 13  [一个简单的逻辑问题，热转100万+，如果真有这么多人，不可能一个热评都进不去，之所以一个都...   \n",
       "9                 11  [蒲地蓝, 你昨天太挑事了，回旋镖了呗, 扁桃体就用抗生素效果最好，不化脓不要用喷剂，基本上...   \n",
       "\n",
       "     user_uid user_name        last_check  check_before  \n",
       "0  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "1  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "2  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "3  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "4  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "5  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "6  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "7  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "8  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  \n",
       "9  6028429521  茶美丽的追剧日常  2023-03-18 09:14             2  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df = pd.DataFrame(all_weibo_content)\n",
    "content_df['user_uid'] = user_id\n",
    "content_df['user_name'] = user_name\n",
    "content_df['last_check'] = time_limit.strftime('%Y-%m-%d %H:%M')\n",
    "content_df['check_before'] = hours\n",
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contains_chinese_keywords(text, keywords):\n",
    "    query_result = []\n",
    "    for keyword in keywords:\n",
    "        # 使用正则表达式查找中文关键词\n",
    "        pattern = re.compile(keyword)\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            query_result.append(keyword)\n",
    "    return query_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"成毅\"]\n",
    "for index,row in content_df.iterrows():\n",
    "    query_keywords_result = contains_chinese_keywords(row['content'],keywords)\n",
    "    if len(query_keywords_result):\n",
    "        print(row['create_at'],row['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
