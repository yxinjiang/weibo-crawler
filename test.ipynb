{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "import re \n",
    "import os \n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import random \n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_followers_id(soup,url_str='https://weibo.cn'):\n",
    "    user_names = [a.get_text() for a in soup.find_all('a', href=True) if url_str in a['href'] and a.get_text()]\n",
    "    user_uids = [a['href'].split('/')[-1] for a in soup.find_all('a', href=True) if url_str in a['href'] and a.get_text()]\n",
    "    user_uids = [item for item in user_uids if not '=' in item or '.' in item]\n",
    "    return user_names, user_uids\n",
    "\n",
    "def save_array2txt(my_array,filename):\n",
    "    # Open the file for writing\n",
    "    with open(filename, 'w') as file:\n",
    "        for item in my_array:\n",
    "        # Write the string representation of the array to the file\n",
    "            file.write(item)\n",
    "            file.write('\\n')\n",
    "    return True\n",
    "\n",
    "def random_wait(min_seconds=3,max_seconds=10):\n",
    "    random_seconds = random.randint(min_seconds, max_seconds)\n",
    "    time.sleep(random_seconds)\n",
    "    print(f'get next page after {random_seconds} seconds')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('cookies.txt', 'r', encoding='utf-8') as f:\n",
    "    cookie = f.read().strip()\n",
    "\n",
    "# 设置请求头部信息\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Cookie': cookie\n",
    "}\n",
    "\n",
    "# 定义需要抓取关注列表的微博用户ID和页数\n",
    "user_id = '6235235201'\n",
    "page = 1\n",
    "user_names = []\n",
    "user_uids = []\n",
    "\n",
    "\n",
    "profile_url = f'https://weibo.cn/{user_id}/profile'\n",
    "profile_response = requests.get(profile_url, headers=headers)\n",
    "profile_soup = BeautifulSoup(profile_response.content, 'html.parser')\n",
    "user_name = profile_soup.find('title').text.split(' ')[0][:-3]\n",
    "print(user_name)\n",
    "\n",
    "\n",
    "\n",
    "# 构造请求URL\n",
    "#url = f'https://weibo.cn/u/{user_id}?page={page}'\n",
    "url = f'https://weibo.cn/{user_id}/follow?page={page}'\n",
    "# 发送请求\n",
    "response = requests.get(url, headers=headers)\n",
    "random_wait(3,4)\n",
    "\n",
    "# 解析HTML页面并提取关注列表\n",
    "soup = BeautifulSoup(response.content, 'html.parser') #html5lib  \n",
    "followings = get_followers_id(soup)\n",
    "user_names += followings[0]\n",
    "user_uids += followings[1]\n",
    "\n",
    "random_wait()\n",
    "\n",
    "page_next = soup.find('a', string='下页')\n",
    "while page_next:\n",
    "    # 获取下一页的URL\n",
    "    page += 1\n",
    "    url = f'https://weibo.cn/{user_id}/follow?page={page}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    followings = get_followers_id(soup)\n",
    "    user_names += followings[0]\n",
    "    user_uids += followings[1]\n",
    "    page_next = soup.find('a', string='下页')\n",
    "    random_wait()\n",
    "\n",
    "user_names = list(set(user_names))\n",
    "user_uids = list(set(user_uids))\n",
    "\n",
    "save_array2txt(user_names,f'./following_lists/{user_id}_{user_name}_followers_names.txt')\n",
    "save_array2txt(user_uids,f'./following_lists/{user_id}_{user_name}_followers_uids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_image(url, filename):\n",
    "    \"\"\"\n",
    "    下载图片\n",
    "    \"\"\"\n",
    "    filename = Path(filename)\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "def download_video(url, path):\n",
    "    \"\"\"\n",
    "    下载视频\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(os.path.join(path, os.path.basename(url)), 'wb') as f:\n",
    "            for chunk in r.iter_content(1024*1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def get_page_content(user_id,page,headers,curr_time,time_limit,user_folder,original_pic_download=False):\n",
    "    today_str = curr_time.strftime('%m月%d日')\n",
    "    next_page = True\n",
    "    # 构造请求URL\n",
    "    url = f'https://weibo.cn/{user_id}?filter=1&page={page}'\n",
    "\n",
    "    # 发送请求\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # 解析HTML页面并提取原创微博\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    weibos = soup.find_all('div', class_='c', id=True)\n",
    "\n",
    "    weibo_content = []\n",
    "\n",
    "    # 输出原创微博\n",
    "    for weibo in weibos:\n",
    "        # 提取微博发布时间\n",
    "        create_time_str = weibo.find('span', class_='ct').get_text()\n",
    "        print(f'create_at: {create_time_str}')\n",
    "        if '分钟前' in create_time_str:\n",
    "            date_str = today_str\n",
    "            minutes = create_time_str.split('分钟前')[0]\n",
    "            if len(minutes)==1:\n",
    "                time_str = f'{curr_time.hour}:0{minutes}'\n",
    "            else:\n",
    "                time_str = f'{curr_time.hour}:{minutes}'\n",
    "        else:\n",
    "            date_str,time_str = create_time_str.split(' ')[0],create_time_str.split(' ')[1]\n",
    "            time_str = time_str[:5]\n",
    "        print('after process: ',date_str,time_str)\n",
    "        if date_str=='今天':\n",
    "            date_str = today_str\n",
    "\n",
    "        time_str = f'{date_str} {time_str}'\n",
    "        time = datetime.strptime(time_str, '%m月%d日 %H:%M')\n",
    "        now = datetime.now()\n",
    "        time = time.replace(year=now.year)\n",
    "        if time > time_limit:\n",
    "            # 提取微博正文\n",
    "            content = weibo.find('span', class_='ctt').get_text()\n",
    "            weibo_content.append({'create_at':time,'content':content})\n",
    "            print(content)\n",
    "            if original_pic_download:\n",
    "                imgs = weibo.find_all('img', alt='图片')\n",
    "                for i,img in enumerate(imgs):\n",
    "                    img_url = img.get('src')\n",
    "                    image_folder = Path(os.path.join(user_folder,'img'))\n",
    "                    image_folder.mkdir(parents=True,exist_ok=True)\n",
    "                    file_name = os.path.join(image_folder,f'{user_id}_{time.strftime(\"%Y%m%d%H%M%S\")}_{i}.jpg')\n",
    "                    download_image(img_url, file_name)\n",
    "\n",
    "        else:\n",
    "            print('time_str: ',time_str)\n",
    "            next_page = False\n",
    "            break\n",
    "        page_next = soup.find('a', string='下页')\n",
    "        if not len(page_next):\n",
    "            next_page = False\n",
    "    return next_page,weibo_content\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "茶美丽的追剧日常\n",
      "get next page after 4 seconds\n",
      "get weibos until 03月17日 15:44\n",
      "create_at: 48分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  03月17日 17:48\n",
      "裤吃不是吃了绝情丹但是媚成毅的❤️已经藏不住了zb就是如此：谁赚钱媚谁[滑稽] \n",
      "create_at: 51分钟前 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  03月17日 17:51\n",
      "#姜广涛涉嫌刑事犯罪#姜广涛如果被列为劣迹人员，他所配音的作品都下架，会有多少人受连累 \n",
      "create_at: 今天 16:28 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  今天 16:28\n",
      "#2022微博十大热门电视剧#刘亦菲，陈晓领衔的梦华录！基本上算去年的剧王了吧？各个榜单都是梦华录第一。 \n",
      "create_at: 今天 16:27 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  今天 16:27\n",
      "#微博之夜第三波官宣阵容#刘宇真滴蛮帅的，他感觉do something了，但又不知道do了哪里，整个人都焕然一新似的，像漫画里走出来的人，你们能get到嘛？ \n",
      "create_at: 今天 16:25 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  今天 16:25\n",
      "#微博白皮书影视作品热门cp#热门CP：顾盼生辉、苍兰夫妇、疑商夫妇、沉香夫妇、命韵峋环。为什么没有余生夫妇？？？？？？？？？ \n",
      "create_at: 今天 16:14 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  今天 16:14\n",
      "#杨紫粉丝娱乐白皮书寄语##微博之夜# 杨紫确定出席2022微博之夜 \n",
      "create_at: 今天 16:09 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  今天 16:09\n",
      "#微博之夜舞台名场面##微博之夜# #群星为娱乐白皮书打call# #微博之夜舞台名场面# 六大舞台名场面：哪个让你们最印象深刻？ \n",
      "create_at: 今天 15:51 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  今天 15:51\n",
      "#迪丽热巴驼色大衣造型##与君初相识播出一周年# 迪丽热巴走御姐风真的yyds，黑长直还蛮有态度的，很干练！偶觉得迪丽热巴和战战有个共同点就是：只要形象气质keep住，就不太会糊，现在的人颜控太多了。 \n",
      "create_at: 今天 15:43 来自沦落民间的iPhone 13(粉色)\n",
      "after process:  今天 15:43\n",
      "time_str:  03月17日 15:43\n",
      "get next page after 16 seconds\n"
     ]
    }
   ],
   "source": [
    "# 设置请求头部信息和Cookie\n",
    "with open('cookies.txt', 'r', encoding='utf-8') as f:\n",
    "    cookie = f.read().strip()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Cookie': cookie\n",
    "}\n",
    "\n",
    "# 定义需要抓取的微博用户ID和页数\n",
    "user_id = '6028429521'\n",
    "original_pic_download=True\n",
    "page = 1\n",
    "hours = 2\n",
    "next_page = True\n",
    "\n",
    "profile_url = f'https://weibo.cn/{user_id}/profile'\n",
    "profile_response = requests.get(profile_url, headers=headers)\n",
    "profile_soup = BeautifulSoup(profile_response.content, 'html.parser')\n",
    "user_name = profile_soup.find('title').text.split(' ')[0][:-3]\n",
    "print(user_name)\n",
    "random_wait(3,4)\n",
    "\n",
    "user_folder = Path(os.path.join(os.getcwd(),'weibo',user_name))\n",
    "user_folder.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# 计算最近n小时的时间\n",
    "curr_time = datetime.now()\n",
    "time_limit = curr_time - timedelta(hours=hours)\n",
    "print('get weibos until',time_limit.strftime('%m月%d日 %H:%M'))\n",
    "today_str = time_limit.strftime('%m月%d日')\n",
    "all_weibo_content = []\n",
    "next_page,weibo_content = get_page_content(user_id,page,headers,curr_time,time_limit,user_folder)\n",
    "all_weibo_content.extend(weibo_content)\n",
    "random_wait(5,20)\n",
    "\n",
    "while(next_page):\n",
    "    # 获取下一页的URL\n",
    "    page += 1\n",
    "    print(f'get content from page {page}')\n",
    "    next_page,weibo_content = get_page_content(user_id,page,headers,curr_time,time_limit,user_folder)\n",
    "    if next_page==False:\n",
    "        break\n",
    "    all_weibo_content.extend(weibo_content)\n",
    "    print(f'find next page {next_page}')\n",
    "    random_wait(5,20)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>create_at</th>\n",
       "      <th>content</th>\n",
       "      <th>user_uid</th>\n",
       "      <th>user_name</th>\n",
       "      <th>last_check</th>\n",
       "      <th>check_before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-17 17:48:00</td>\n",
       "      <td>裤吃不是吃了绝情丹但是媚成毅的❤️已经藏不住了zb就是如此：谁赚钱媚谁[滑稽]</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-17 17:51:00</td>\n",
       "      <td>#姜广涛涉嫌刑事犯罪#姜广涛如果被列为劣迹人员，他所配音的作品都下架，会有多少人受连累</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-17 16:28:00</td>\n",
       "      <td>#2022微博十大热门电视剧#刘亦菲，陈晓领衔的梦华录！基本上算去年的剧王了吧？各个榜单都是...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-17 16:27:00</td>\n",
       "      <td>#微博之夜第三波官宣阵容#刘宇真滴蛮帅的，他感觉do something了，但又不知道do了...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-17 16:25:00</td>\n",
       "      <td>#微博白皮书影视作品热门cp#热门CP：顾盼生辉、苍兰夫妇、疑商夫妇、沉香夫妇、命韵峋环。为...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-03-17 16:14:00</td>\n",
       "      <td>#杨紫粉丝娱乐白皮书寄语##微博之夜# 杨紫确定出席2022微博之夜</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-03-17 16:09:00</td>\n",
       "      <td>#微博之夜舞台名场面##微博之夜# #群星为娱乐白皮书打call# #微博之夜舞台名场面# ...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-03-17 15:51:00</td>\n",
       "      <td>#迪丽热巴驼色大衣造型##与君初相识播出一周年# 迪丽热巴走御姐风真的yyds，黑长直还蛮有...</td>\n",
       "      <td>6028429521</td>\n",
       "      <td>茶美丽的追剧日常</td>\n",
       "      <td>2023-03-17 15:44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            create_at                                            content  \\\n",
       "0 2023-03-17 17:48:00           裤吃不是吃了绝情丹但是媚成毅的❤️已经藏不住了zb就是如此：谁赚钱媚谁[滑稽]    \n",
       "1 2023-03-17 17:51:00       #姜广涛涉嫌刑事犯罪#姜广涛如果被列为劣迹人员，他所配音的作品都下架，会有多少人受连累    \n",
       "2 2023-03-17 16:28:00  #2022微博十大热门电视剧#刘亦菲，陈晓领衔的梦华录！基本上算去年的剧王了吧？各个榜单都是...   \n",
       "3 2023-03-17 16:27:00  #微博之夜第三波官宣阵容#刘宇真滴蛮帅的，他感觉do something了，但又不知道do了...   \n",
       "4 2023-03-17 16:25:00  #微博白皮书影视作品热门cp#热门CP：顾盼生辉、苍兰夫妇、疑商夫妇、沉香夫妇、命韵峋环。为...   \n",
       "5 2023-03-17 16:14:00                #杨紫粉丝娱乐白皮书寄语##微博之夜# 杨紫确定出席2022微博之夜    \n",
       "6 2023-03-17 16:09:00  #微博之夜舞台名场面##微博之夜# #群星为娱乐白皮书打call# #微博之夜舞台名场面# ...   \n",
       "7 2023-03-17 15:51:00  #迪丽热巴驼色大衣造型##与君初相识播出一周年# 迪丽热巴走御姐风真的yyds，黑长直还蛮有...   \n",
       "\n",
       "     user_uid user_name        last_check  check_before  \n",
       "0  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  \n",
       "1  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  \n",
       "2  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  \n",
       "3  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  \n",
       "4  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  \n",
       "5  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  \n",
       "6  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  \n",
       "7  6028429521  茶美丽的追剧日常  2023-03-17 15:44             2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df = pd.DataFrame(all_weibo_content)\n",
    "content_df['user_uid'] = user_id\n",
    "content_df['user_name'] = user_name\n",
    "content_df['last_check'] = time_limit.strftime('%Y-%m-%d %H:%M')\n",
    "content_df['check_before'] = hours\n",
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contains_chinese_keywords(text, keywords):\n",
    "    query_result = []\n",
    "    for keyword in keywords:\n",
    "        # 使用正则表达式查找中文关键词\n",
    "        pattern = re.compile(keyword)\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            query_result.append(keyword)\n",
    "    return query_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-17 17:48:00 裤吃不是吃了绝情丹但是媚成毅的❤️已经藏不住了zb就是如此：谁赚钱媚谁[滑稽] \n"
     ]
    }
   ],
   "source": [
    "keywords = [\"成毅\"]\n",
    "for index,row in content_df.iterrows():\n",
    "    query_keywords_result = contains_chinese_keywords(row['content'],keywords)\n",
    "    if len(query_keywords_result):\n",
    "        print(row['create_at'],row['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
